IntelliBid — The AI-Powered Tech & Vendor Shortlisting Engine
Problem:
Evaluating RFT/RFI responses and partner proposals is manual and inconsistent. Different teams (Delivery, Procurement, Product, Architecture, Engineering, QA) must interpret complex technical, commercial, and compliance criteria—causing delays, bias, and poor traceability in why options were shortlisted or rejected.
Solution:
IntelliBid is an AI system that understands both requirements and proposals, then creates a transparent, risk-adjusted shortlisting with clear rationale. It serves Delivery Managers, Procurement, Product Owners, Architects, Developers, and Testers by aligning technical fit, delivery risk, cost, and compliance in one view.
How it works:
Requirement Parsing: Extracts scope, technical NFRs, success metrics, and evaluation weights from RFT/BRD/EPICs.
Proposal Understanding: Converts vendor/partner docs (PDF/Word/Excel) into structured, comparable data.
Semantic Matching: Maps capabilities to requirements (features, integrations, SLAs, security, data/privacy).
Scoring & Ranking: Applies weighted scoring (tech fit, delivery risk, cost, compliance, team capability), using historical performance where available.
Bias Mitigation: Enforces objective metrics and explains trade-offs.
Role-Aware Views:
Delivery/PMO: timeline risk, dependencies, resourcing fit
Procurement/Finance: TCO, commercials, SLA history
Product: feature coverage, roadmap fit
Architecture: standards compliance, integration complexity, security posture
Developers/QA: SDKs/APIs, testability, tooling, documentation quality
Shortlisting Report: Ranked options with rationale, gaps, mitigation plan, and recommended next steps.
Innovation:
Embeds enterprise knowledge—standards, patterns, past project outcomes, vendor performance, and cost models—so the AI reasons like a cross-functional evaluation panel, not just a procurement tool.
Hackathon MVP:
Given a requirements pack and 2–3 sample proposals, the system generates an AI-driven Shortlisting Report with:
Fit % per option, weighted scores, and explanation
Role-specific summaries (Delivery, Product, Arch, Eng, QA, Procurement)
Visual comparisons and recommended next step
Runs in a sandbox with synthetic data.
Technology & Tools Summary
Core Technologies:
LLM Reasoning: Azure OpenAI (GPT-4o) for extraction, comparison, and explanations
Vector KB: Chroma/FAISS seeded with standards, patterns, prior evaluations
Orchestration: LangChain or CrewAI (agents for parsing, scoring, reporting)
Frontend: React UI with Power BI/Charts for dashboards
Connectors: MCP to Vendor DB, SLA history, finance ledgers, architecture standards
Expected Outcome:
A working POC that turns days of multi-team review into minutes of objective, explainable shortlisting, giving Delivery, Product, Architecture, Engineering, QA, and Procurement a shared, auditable decision framework.